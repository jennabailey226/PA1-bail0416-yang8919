// This autogenerated skeleton file illustrates how to build a server.
// You should copy it to another filename to avoid overwriting it.

#include "./gen-cpp/Coordinator.h"
#include "./gen-cpp/ComputeNode.h"
#include <thrift/protocol/TBinaryProtocol.h>
#include <thrift/server/TSimpleServer.h>
#include <thrift/transport/TServerSocket.h>
#include <thrift/transport/TBufferTransports.h>
#include <thrift/transport/TSocket.h>
#include <thrift/transport/TTransportUtils.h>
#include <thread>
#include <queue>
#include <sstream>
#include <list>
#include <vector>
#include "./ML/ML/ML.hpp"

using namespace ::apache::thrift;
using namespace ::apache::thrift::protocol;
using namespace ::apache::thrift::transport;
using namespace ::apache::thrift::server;

using namespace  ::distributedML;

class CoordinatorHandler : virtual public CoordinatorIf {
 public:
  CoordinatorHandler() {
    // Your initialization goes here
  }

  void load_compute_nodes(std::vector<ComputeNodeInfo> & _return, const std::string& filename) {
    std::ifstream file(filename);
    if (!file.is_open()) {
      std::cout << "Error: Unable to open file " << filename << std::endl;
      exit(1);
    }
    std::string line;
    while (getline(file, line)) {
      ComputeNodeInfo in;
      std::stringstream ss(line);
      if (!std::getline(ss, in.ip, ',')) {
          std::cout << "Error: Invalid format in file (IP missing): " << line << std::endl;
          exit(1);
      }
      string port_str;
      if (!std::getline(ss, port_str, ',')) {
          std::cout << "Error: Invalid format in file (Port missing): " << line << std::endl;
          exit(1);
      }
      string load_prob_str;
      if (!std::getline(ss, load_prob_str)) {
          std::cout << "Error: Invalid format in file (Load probability missing): " << line << std::endl;
          exit(1);
      }
      in.port = std::stoi(port_str);
      in.load_probability = std::stod(load_prob_str);
      _return.emplace_back(in);
    }
    file.close();
  }

  double train(const std::string& dir, const int32_t rounds, const int32_t epochs, const int32_t h, const int32_t k, const double eta, const std::string& compute_nodes_file) {
    std::cout << "Coordinator: rounds: " << rounds << " epocks: " << epochs << " hidden unit: "
      << h << " output unit: " << k << " learning rate: " << eta << std::endl;

    // Initialize the global model
    mlp almighty;

    // filename's path is for ML.cpp
    if (!almighty.init_training_random(dir + "/ML/ML/letters/train_letters1.txt", k, h)){
      std::cout << "Can't open the training file\n";
      exit(1);
    }

    Weights shared_weights;
    // std::vector<std::vector<double>> W;
    // std::vector<std::vector<double>> V;

    almighty.get_weights(shared_weights.V, shared_weights.W);

    // std::cout << "v's size" << V.size() << endl;
    // std::cout << "w's size" << W.size() << endl;

    queue<std::string> work_queue;
    for (int i = 1; i < 2; i++) {
        work_queue.push(dir + "/ML/ML/letters/train_letters" + std::to_string(i) + ".txt");
    }

    std::vector<ComputeNodeInfo> compute_nodes;
    std::cout << "before load_compute_nodes\n";
    load_compute_nodes(compute_nodes, dir + "/" + compute_nodes_file);
    if (compute_nodes.empty()) {
      std::cout << "Error: No compute nodes found in " << compute_nodes_file << std::endl;
      exit(1);
    }
    std::cout << "after load_compute_nodes\n";

    std::vector<vector<double>> shared_gradient_V(h + 1, vector<double>(k, 0));
    std::vector<vector<double>> shared_gradient_W(17, vector<double>(h, 0));

    // thread function
    auto thread_func = [&](int node_index) {
      while (true){
        if (work_queue.empty()) {
          return;
        }
        std::string training_file = work_queue.front();
        work_queue.pop();
        std::shared_ptr<TTransport> socket(new TSocket(compute_nodes[node_index].ip, compute_nodes[node_index].port));
        std::shared_ptr<TTransport> transport(new TBufferedTransport(socket));
        std::shared_ptr<TProtocol> protocol(new TBinaryProtocol(transport));
        ComputeNodeClient client(protocol);

        transport->open();
        Gradient grad;
        client.trainModel(grad, shared_weights, training_file, eta, epochs);
        transport->close();
        std::cout << "coordinator1\n";
        for (int i = 0; i < shared_gradient_V.size(); i++) {
          for (int j = 0; j < shared_gradient_V[i].size(); j++) {
              shared_gradient_V[i][j] += grad.dV[i][j];
          }
        }
        for (int i = 0; i < shared_gradient_W.size(); i++) {
          for (int j = 0; j < shared_gradient_W[i].size(); j++) {
              shared_gradient_W[i][j] += grad.dW[i][j];
          }
        }
      }
    };
    std::cout << "coordinator2\n";
    std::vector<std::thread> workers;
    for (int i = 0; i < compute_nodes.size(); i++) {
        workers.emplace_back(thread_func, i);
    }
    std::cout << "coordinator3\n";
    for (auto& worker:workers) {
        worker.join();
    }
    std::cout << "coordinator4\n";
    almighty.update_weights(shared_gradient_V, shared_gradient_W);
    std::cout << "coordinator5\n";
    double validation_error = almighty.validate(dir + "/ML/ML/letters/validate_letters.txt");
    std::cout << "coordinator6\n";
    // std::cout << "Validation error: " << validation_error << endl;
    return validation_error;
  }

};

int main(int argc, char **argv) {
  if (argc != 3) {
      std::cout << "Usage: ./coordinator <port> <scheduling_policy>" << std::endl;
      exit(1);
  }
  int port = std::stoi(argv[1]);
  std::string policy = argv[2];
  ::std::shared_ptr<CoordinatorHandler> handler(new CoordinatorHandler());
  ::std::shared_ptr<TProcessor> processor(new CoordinatorProcessor(handler));
  ::std::shared_ptr<TServerTransport> serverTransport(new TServerSocket(port));
  ::std::shared_ptr<TTransportFactory> transportFactory(new TBufferedTransportFactory());
  ::std::shared_ptr<TProtocolFactory> protocolFactory(new TBinaryProtocolFactory());

  TSimpleServer server(processor, serverTransport, transportFactory, protocolFactory);
  server.serve();
  return 0;
}
