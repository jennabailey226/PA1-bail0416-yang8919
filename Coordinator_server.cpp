// This autogenerated skeleton file illustrates how to build a server.
// You should copy it to another filename to avoid overwriting it.

#include "./gen-cpp/Coordinator.h"
#include "./gen-cpp/ComputeNode.h"
#include <thrift/protocol/TBinaryProtocol.h>
#include <thrift/server/TSimpleServer.h>
#include <thrift/transport/TServerSocket.h>
#include <thrift/transport/TBufferTransports.h>
#include <thrift/transport/TSocket.h>
#include <thrift/transport/TTransportUtils.h>
#include <thread>
#include <queue>
#include <sstream>
#include <list>
#include <vector>
#include "./ML/ML/ML.hpp"
#define FEATURES 17

using namespace ::apache::thrift;
using namespace ::apache::thrift::protocol;
using namespace ::apache::thrift::transport;
using namespace ::apache::thrift::server;

using namespace  ::distributedML;

class CoordinatorHandler : virtual public CoordinatorIf {
 public:
  CoordinatorHandler() {
    // Your initialization goes here
  }

  void load_compute_nodes(std::vector<ComputeNodeInfo> & _return, const std::string& filename) {
    std::ifstream file(filename);
    if (!file.is_open()) {
      std::cout << "Error: Unable to open file " << filename << std::endl;
      exit(1);
    }
    std::string line;
    while (getline(file, line)) {
      ComputeNodeInfo in;
      std::stringstream ss(line);
      if (!std::getline(ss, in.ip, ',')) {
          std::cout << "Error: Invalid format in file (IP missing): " << line << std::endl;
          exit(1);
      }
      string port_str;
      if (!std::getline(ss, port_str, ',')) {
          std::cout << "Error: Invalid format in file (Port missing): " << line << std::endl;
          exit(1);
      }
      string load_prob_str;
      if (!std::getline(ss, load_prob_str)) {
          std::cout << "Error: Invalid format in file (Load probability missing): " << line << std::endl;
          exit(1);
      }
      in.port = std::stoi(port_str);
      in.load_probability = std::stod(load_prob_str);
      _return.emplace_back(in);
    }
    file.close();
  }

  double train(const std::string& dir, const int32_t rounds, const int32_t epochs, const int32_t h, const int32_t k, const double eta, const std::string& compute_nodes_file) {
    std::cout << "Coordinator: rounds: " << rounds << " epocks: " << epochs << " hidden unit: "
      << h << " output unit: " << k << " learning rate: " << eta << std::endl;

    // initialize the model
    mlp almighty;

    // filename's path is for ML.cpp
    if (!almighty.init_training_random(dir + "/ML/ML/letters/train_letters1.txt", k, h)){
      std::cout << "Can't open the training file\n";
      exit(1);
    }

    Weights shared_weights;
    almighty.get_weights(shared_weights.V, shared_weights.W);

    // load compute nodes
    std::vector<ComputeNodeInfo> compute_nodes;
    load_compute_nodes(compute_nodes, dir + "/" + compute_nodes_file);
    if (compute_nodes.empty()) {
      std::cout << "Error: No compute nodes found in " << compute_nodes_file << std::endl;
      exit(1);
    }

    double validation_error;
    for (int i = 0; i < rounds; i++){
      std::vector<std::vector<double>> shared_gradient_W(FEATURES, std::vector<double>(h, 0));
      std::vector<std::vector<double>> shared_gradient_V(h + 1, std::vector<double>(k, 0));

      queue<std::string> work_queue;
      // thread function
      auto thread_func = [&](int node_index) {
        while (true){
          if (work_queue.empty()) {
            return;
          }
          std::string training_file = work_queue.front();
          work_queue.pop();

         //Section generated by OpenAI
          std::shared_ptr<TTransport> socket(new TSocket(compute_nodes[node_index].ip, compute_nodes[node_index].port));
          std::shared_ptr<TTransport> transport(new TBufferedTransport(socket));
          std::shared_ptr<TProtocol> protocol(new TBinaryProtocol(transport));
          ComputeNodeClient client(protocol);

          transport->open();
          Gradient grad;
          client.trainModel(grad, shared_weights, training_file, eta, epochs);
          transport->close();
         //end section

          for (int i = 0; i < shared_gradient_V.size(); i++) {
            for (int j = 0; j < shared_gradient_V[i].size(); j++) {
                shared_gradient_V[i][j] += grad.dV[i][j];
            }
          }
          for (int i = 0; i < shared_gradient_W.size(); i++) {
            for (int j = 0; j < shared_gradient_W[i].size(); j++) {
                shared_gradient_W[i][j] += grad.dW[i][j];
            }
          }
        }
      };

      // training data, later will change it to i<12
      for (int i = 1; i < 12; i++) {
          work_queue.push(dir + "/ML/ML/letters/train_letters" + std::to_string(i) + ".txt");
      }

      std::vector<std::thread> workers;
      for (int i = 0; i < compute_nodes.size(); i++) {
          workers.emplace_back(thread_func, i);
      }

      for (auto& worker:workers) {
          worker.join();
      }

      almighty.update_weights(shared_gradient_V, shared_gradient_W);

      validation_error = almighty.validate(dir + "/ML/ML/letters/validate_letters.txt");
      std::cout << "validation error: " << validation_error << std::endl;
    }
    return validation_error;
  }

};

int main(int argc, char **argv) {
  if (argc != 3) {
      std::cout << "Usage: ./coordinator <port> <scheduling_policy>" << std::endl;
      exit(1);
  }
  int port = std::stoi(argv[1]);
  std::string policy = argv[2];
  ::std::shared_ptr<CoordinatorHandler> handler(new CoordinatorHandler());
  ::std::shared_ptr<TProcessor> processor(new CoordinatorProcessor(handler));
  ::std::shared_ptr<TServerTransport> serverTransport(new TServerSocket(port));
  ::std::shared_ptr<TTransportFactory> transportFactory(new TBufferedTransportFactory());
  ::std::shared_ptr<TProtocolFactory> protocolFactory(new TBinaryProtocolFactory());

  TSimpleServer server(processor, serverTransport, transportFactory, protocolFactory);
  server.serve();
  return 0;
}
